
% This file should be compiled with V2.5 of "sig-alternate.cls" May 2012
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.5) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V2.0 - May 2012

\documentclass[runningheads,a4paper]{llncs}

\usepackage{xspace}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{epsfig}
\usepackage{amsmath}
\usepackage{cite}
\usepackage[hidelinks]{hyperref}
\newcommand*{\eg}{e.g.\@\xspace}
\newcommand*{\ie}{i.e.\@\xspace}
\usepackage{color}
\usepackage{microtype}

\setlength{\intextsep}{5pt plus 0pt minus 0pt}
\def\baselinestretch{0.99}

\newcommand{\longpage}{\enlargethispage{\baselineskip}}
\newcommand{\verylongpage}{\enlargethispage{2\baselineskip}}

\begin{document}

% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Ethnicity sensitive author disambiguation using semi-supervised learning}

\authorrunning{G. Louppe, H. Al-Natsheh, M. Susik, E. Maguire}
\author{Gilles Louppe\inst{1}\and Hussein T. Al-Natsheh\inst{2}\and Mateusz Susik\inst{3}\and\\ 
Eamonn James Maguire\inst{1}}

\institute{CERN, Switzerland,\\
\email{\{g.louppe, eamonn.james.maguire\}@cern.ch},
\and
ISH-LYON (USR 3385), CNRS, France,\\
\email{hussein.al-natsheh@ish-lyon.cnrs.fr}
\and
University of Warsaw, Poland,\\
\email{msusik@student.uw.edu.pl}
}


\maketitle
\begin{abstract}

Author name disambiguation in bibliographic databases is the problem of grouping
together scientific publications written by the same person, accounting for
potential homonyms and/or synonyms. Among solutions to this problem, digital
libraries are increasingly offering tools for authors to manually curate their
publications and claim those that are theirs. Indirectly, these tools allow for
the inexpensive collection of large annotated training data, which can be
further leveraged to build a complementary automated disambiguation system
capable of inferring patterns for identifying publications written by the same
person. Building on more than 1 million crowdsourced
annotations that we publicly release, we propose an automated author disambiguation solution exploiting
this data (i) to learn an accurate classifier for identifying coreferring
authors and (ii) to guide the clustering of scientific publications by distinct
authors in a semi-supervised way. To the best of our knowledge, our analysis is
the first to be carried out on data of this size and coverage. With respect to
the state of the art, we validate the general pipeline used in most existing
solutions, and improve by: (i) proposing phonetic-based blocking strategies,
thereby increasing recall; (ii) adding strong ethnicity-sensitive features for
learning a linkage function, thereby tailoring disambiguation to non-Western
author names whenever necessary; and (iii) showing the importance of balancing
negative and positive examples when learning the linkage function.

\end{abstract}

\section{Introduction}
In academic digital libraries, author name disambiguation is the problem of
grouping together publications written by the same person.
It is often difficult because an author may use different
spellings or name variants across their career (synonymy) and/or distinct authors may
share the same name (polysemy).
Most notably, author disambiguation is often more troublesome for researchers
from non-Western cultures, where personal names may be traditionally less diverse (leading to
homonym issues) or for which transliteration to Latin characters may not be unique (leading to
synonym issues).
With the fast growth of the scientific literature, author disambiguation has become a pressing
issue since the accuracy of information managed at the level of individuals directly affects:
the relevance search of results (\eg, when querying for all publications written by a given author);
the reliability of bibliometrics and author rankings (\eg, citation counts or other impact
metrics, as studied in \cite{strotmann2012author}); and/or the relevance of scientific network analysis
\cite{newman2001structure}. Thus, even small improvements in the field significantly improve the usability of the digital libraries to some users.

Solutions to author disambiguation have been proposed from various
communities \cite{liu2014author}. On the one hand, libraries have maintained
authorship control through manual curation, either in a centralized way by
hiring professional collaborators or through developing services that invite authors to register
their publications themselves (\eg, Google Scholar or Inspire-HEP).
Recent efforts to create persistent digital identifiers assigned to researchers (\eg, ORCID or ResearcherID),
with the objective to embed these identifiers in the submission workflow of publishers
or repositories (\eg, Elsevier, arXiv or Inspire-HEP), would univocally solve any disambiguation issue.
As the centralized manual authorship control is expensive and the success of persistent digital
identifiers requires large and ubiquitous adoption by both researchers and publishers, fully
automated machine learning-based methods have been proposed to
provide immediate, less costly, and satisfactory solutions to author
disambiguation. In this work, we study how labeled data obtained through manual curation (either centralized or
crowdsourced) can be exploited (i) to learn an accurate classifier for
identifying coreferring authors, and (ii) to guide the clustering of scientific
publications by distinct authors in a semi-supervised way.
Our analysis of parameters and features of this large dataset reveal that the general pipeline
commonly used in existing solutions is an effective approach for author disambiguation.
Moreover, we propose better strategies for blocking based on the
phonetization of author names to increase recall and ethnicity-sensitive features for learning a
linkage function which tailor our author disambiguation to non-Western author names.


The remainder of this report is structured as follows. In Section~\ref{related-works},
we briefly review machine learning solutions for author disambiguation. The components of our method are then defined in Section~\ref{methods} and its implementation described in Section~\ref{implementation}. Experiments
are carried out in Section~\ref{experiments}, where we compare
approaches to the problem and explore feature choice.
Finally, conclusions and future works are discussed in Section~\ref{conclusions}.

% Related works ==================================================================

\section{Related work}
\label{related-works}

\longpage

As reviewed in \cite{smalheiser2009author,ferreira2012brief,levin2012citation}, author
disambiguation algorithms are usually composed of two main components: (i) a
linkage function determining whether two publications have been written by the
same author; and (ii) a clustering algorithm producing clusters of publications
assumed to be written by the same author.
Approaches can be classified along several axes, depending on the type and
amount of data available, the way the linkage function is learned or defined, or the
clustering procedure used to group publications.
Methods relying on supervised learning usually make use of a small set of hand-labeled pairs
of publications identified as being either from the same or different authors to automatically learn a linkage
function between publications \cite{han2004two,huang2006efficient, culotta2007author,treeratpituk2009disambiguating,tran2014author}.

Training data is usually not easily available, therefore unsupervised approaches propose
the use of domain-specific, manually designed, linkage functions tailored towards author
disambiguation \cite{malin2005unsupervised,song2007efficient, kang2009co,schulz2014exploiting}.
These approaches have the advantage of not requiring hand-labeled data, but generally do
not perform as well as supervised approaches.
To reconcile both worlds, semi-supervised methods make use of small, manually verified clusters of
publications and/or high-precision domain-specific rules to build a training
set of pairs of publications, from which a linkage function is then built using supervised learning
\cite{ferreira2010effective,torvik2009author,levin2012citation}. Semi-supervised approaches also allow for the tuning of the
clustering algorithm when the latter is applied to a mixed set of labeled
and unlabeled publications, \eg, by maximizing some clustering performance
metric on the known clusters \cite{levin2012citation}.

%Due to the lack of large and publicly available datasets of curated
%clusters of publications, studies on author disambiguation are usually
%constrained to validating their results on manually built datasets of limited
%size and scope (from a few hundred to a few thousand papers, with sparse
%coverage of ambiguous cases), making the true performance of these methods
%often difficult to assess with high confidence.
%Additionally, despite devoted efforts to construct them, these datasets are rarely public,
%making it even more difficult to compare methods using a common benchmark.

In this context, we position this work as a
semi-supervised solution for author disambiguation, with the significant
advantage of having a very large collection of more than 1 million crowdsourced annotations
of publications whose true authors are identified.
The extent and coverage of this data allows us to revisit, validate and nuance previous
findings regarding supervised learning of linkage functions, and to better explore strategies
for semi-supervised clustering.
Furthermore, by releasing our data in the public domain, we provide a benchmark on
which further research on author disambiguation and related topics can be evaluated.


% Methods =====================================================================

\section{Semi-supervised author disambiguation}
\label{methods}

Formally, let us assume a set of publications ${\cal P} = \{ p_0, ...,
p_{N-1}\}$ along with the set of unique individuals ${\cal A} = \{ a_0, ...,
a_{M-1}\}$ having together authored all publications in ${\cal P}$.  Let us
define a signature $s \in p$ from a publication as a unique piece of
information identifying one of the authors of $p$ (\eg, the author name, his
affiliation, along with any other metadata that can be derived from $p$, as illustrated in Figure~\ref{fig:signature}). Let us
denote by ${\cal S} = \{ s | s \in p, p \in {\cal P} \}$ the set of all
signatures that can be extracted from all publications in ${\cal P}$.

\begin{figure*}
\centering
\includegraphics[width=\textwidth]{fig-pub-to-signature.pdf}
\caption{An example signature $s$ for ``Doe, John". A \textit{signature} is
defined as unique piece of information identifying an author on a publication,
along with any other metadata that can be derived from it, such as publication
title, co-authors or date of publication.}
\label{fig:signature}
\end{figure*}

Author disambiguation can be stated as the problem of
finding a partition ${\cal C} = \{ c_0, ..., c_{M-1} \}$ of ${\cal S}$ such
that ${\cal S} = \cup_{i=0}^{M-1} c_i$, $c_i \cap c_j = \phi$ for all $i \neq
j$, and where subsets $c_i$, or clusters, each corresponds to the set of all
signatures belonging to the same individual $a_i$. Alternatively, the set
${\cal A}$ may remain (possibly partially) unknown, such that author
disambiguation boils down to finding a partition ${\cal C}$ where
subsets $c_i$  each correspond to the set of all signatures from the same
individual (without knowing who). Finally, in the case of partially annotated databases as studied in
this work, the set extends with the partial knowledge ${\cal C}^\prime = \{ c_0^\prime, ..., c_{M-1}^\prime \}$ of ${\cal C}$,
such that $c_i^\prime \subseteq c_i$, where $c_i^\prime$ may be empty.

The distinctive aspect of our work is the
knowledge of more than 1 million crowdsourced annotations, indicating together that all signature $s \in
c_i^\prime$ are known to correspond to the same individual $a_i$.

\longpage

Our algorithm is composed of three parts (Figure~\ref{fig:workflow}): (i) a blocking
scheme whose goal is to pre-cluster signatures ${\cal S}$ into smaller groups; (ii) the construction of a linkage function
$d$ between signatures using supervised learning; and (iii) the
semi-supervised clustering of all signatures within the same block, using $d$ as a pseudo distance metric.


\begin{figure*}
\centering
\includegraphics[width=\textwidth]{fig-workflow.pdf}
\caption{Pipeline for author disambiguation: (a)
signatures are \textit{blocked} to reduce computational complexity, (b) a linkage
function is built with supervised learning, (c) independently within each block, signatures
are grouped using hierarchical agglomerative clustering.}
\label{fig:workflow}
\end{figure*}

\subsection{Blocking}
\label{methods:blocking}

As in previous works, the first part of our algorithm consists of dividing
signatures ${\cal S}$ into disjoint subsets ${\cal S}_{b_0}, ..., {\cal
S}_{b_{K-1}}$, or \textit{blocks} \cite{fellegi69}, followed by carrying out
author disambiguation on each one of these blocks independently.
By doing so, the computational complexity of clustering (see Section~\ref{methods:clustering})
typically reduces from $O(|{\cal S}|^2)$ to $O(\sum_b |{\cal S}_b|^2)$.
Since disambiguation is performed independently per block, a good blocking strategy should be
designed such that signatures from the same author are all mapped to the same
block, otherwise their correct clustering would not be possible in later stages of the workflow.
As a result, blocking should be a balance between reduced complexity and maximum recall.

The simplest and most common strategy for blocking, referred to hereon in as \textit{Surname and First Initial (SFI)},
groups signatures together if they share the same surname(s) and the same first
given name initial.
Despite satisfactory performance, there are several cases where this simple strategy fails to cluster related pairs of signatures together, including:

\begin{enumerate}
  \item There are different
  ways of writing an author name, or signatures contain a typo
  (\eg, ``Mueller, R." and ``Muller, R.").

  \item An author has multiple surnames and some signatures place the first part of the surname within the given names (\eg, ``Martinez Torres, A." and ``Torres, A. Martinez").

  \item An author has multiple surnames and, on some signatures, only the first surname is
  present (\eg, ``Smith-Jones, A." and ``Smith, A.")

  \item An author has multiple given names and they are not always all recorded (\eg,
  ``Smith, Jack" and ``Smith, A. J.")

  \item An authors surname changed (\eg, due to marriage).
\end{enumerate}


To account for these issues we propose instead to block signatures based on the
phonetic representation of the normalized surname.
Normalization involves stripping accents (\eg, ``Jab\l{}o\'nski, \L{}" $\rightarrow$ ``Jablonski, L") and name
affixes that inconsistently appear in signatures (\eg, ``van der Waals, J. D."
$\rightarrow$ ``Waals, J. D."), while phonetization is based either on the
Double Metaphone \cite{doublemetaphone}, the NYSIIS \cite{nysiis} or the
Soundex \cite{Soundex} phonetic algorithms for mapping author names to their pronunciations.
Together, these processing steps allow for grouping of most name variants of the same
person in the same block with a small increase in the overall computational complexity, thereby solving case 1.

In the case of multiple surnames (cases 2 and 3), we propose to block
signatures in two phases.
In the first phase, all the signatures with a single surname are clustered together.
Every different surname token creates a new block.
In the second phase, the signatures with multiple surnames are compared
with the blocks for the first and last surname.
If the first surnames of an author were already used as the last given names on some of the signatures, the
new signature is assigned to the block of the last surname (case 2).
Otherwise, the signature is assigned to the block of the first surname (case 3).
Finally, to prevent the creation of too large blocks, signatures are further divided
along their first given name initial.
Cases 4 and 5 are not explicitly handled.

\subsection{Linkage function}
\label{methods:linkage}

\subsubsection{Supervised classification.} The second part of the algorithm is the
automatic construction of a pair-wise linkage function between signatures for use
during the clustering step which groups all signatures from the same author.

Formally, the goal is to build a function $d: {\cal S} \times {\cal S} \mapsto
[0, 1]$, such that $d(s_1, s_2)$ approaches $0$ if both signatures $s_1$ and
$s_2$ belong to the same author, and $1$ otherwise.
This problem can be cast as a supervised classification task, where inputs
are pairs of signatures and outputs are classes $0$ (same authors), and $1$
(distinct authors). In this work, we evaluate Random Forests (RF, \cite{breiman2001random}),
Gradient Boosted Regression Trees (GBRT, \cite{friedman2001greedy}),
and Logistic Regression \cite{fan2008liblinear} as classifiers.


%In most cases, supervised learning algorithms assume
%the input space ${\cal X}$ to be numeric (\eg, $\mathbb{R}^p$), making them
%not directly applicable to structured input spaces such as ${\cal S} \times
%{\cal S}$.

\subsubsection{Input features.} Following previous works, pairs of signatures $(s_1, s_2)$ are first transformed to vectors $v \in \mathbb{R}^p$
by building so-called similarity profiles \cite{treeratpituk2009disambiguating} on which supervised learning is carried out.
In this work, we design and evaluate fifteen standard input
features based on the comparison of signature fields, as reported in the first
half of Table~\ref{table:features}.
As an illustrative example, the \textit{Full name} feature corresponds to the similarity between the (full)
author name fields of the two signatures, as measured using as combination
operator the cosine similarity between their respective $(n,m)$-\emph{TF-IDF} vector
representations\footnote{$(n,m)-$\emph{TF-IDF} vectors are \emph{TF-IDF} vectors computed
from $n$, $n+1$, ..., $m$-grams.}.

Authors from different origins or ethnic groups are likely to be
disambiguated using different strategies (\eg, pairs of signatures with French
author names versus pairs of signatures with Chinese author names)
\cite{treeratpituk2012name, chin2014effective}. For example, scientist coming from China
might more/less often change affiliations, and this dependency, if learnt by the classifier, 
should improve the fit.
To support our disambiguation algorithm, we added seven features to our feature set, with each
evaluating the degree of belonging of both signatures to an ethnic group.

\longpage

More specifically, using census data extracted from \cite{rugglesintegrated},
we build a support vector machine classifier (using a linear kernel and
one-versus-all classification scheme) for mapping the
$(1,5)$-TF-IDF representation of an author name to one of the ethnic groups, as
defined in United States federal censuses. These groups are: \textit{White},
\textit{Black of African American}, \textit{American Indian and Alaska Native},
\textit{Asian}, \textit{Native Hawaiian and Other Pacific Islander},
\textit{Japanese}, \textit{Chinese}, \textit{Others}. Given a
pair of signatures $(s_1, s_2)$, the proposed ethnicity features are each
computed as the estimated probability of $s_1$ belonging to the corresponding
ethnic group, multiplied by the estimated probability of $s_2$ belonging to the
same group. Each of the seven races is used to create a single new feature.
In doing so, the expectation is for the linkage function to become
sensitive to the actual origin of the authors depending on the values of these
features. Indirectly, these features also hold discriminative
power since if author names are predicted to belong to different ethnic groups,
then they are also likely to correspond to distinct people.

\begin{table*}
\caption{Input features for learning a linkage function}
\label{table:features}
\centering
{\small
\begin{tabular}{|l|l|}
  \hline
  \textbf{Feature} & \textbf{Combination operator}\\
  \hline
  \hline
  Full name & Cosine similarity of $(2,4)$-TF-IDF\\
  Given names & Cosine similarity of $(2,4)$-TF-IDF\\
  First given name & Jaro-Winkler distance\\
  Second given name & Jaro-Winkler distance\\
  Given name initial & Equality\\
  Affiliation & Cosine similarity of $(2,4)$-TF-IDF\\
  Co-authors & Cosine similarity of TF-IDF\\
  Title & Cosine similarity of $(2,4)$-TF-IDF\\
  Journal & Cosine similarity of $(2,4)$-TF-IDF\\
  Abstract & Cosine similarity of TF-IDF\\
  Keywords & Cosine similarity of TF-IDF\\
  Collaborations & Cosine similarity of TF-IDF\\
  References & Cosine similarity of TF-IDF\\
  Subject & Cosine similarity of TF-IDF\\
  Year difference & Absolute difference\\
  \hline
  Any ethnicity feature & Product of probabilities estimated by SVM\\
  \hline
\end{tabular}}
\end{table*}

%\glnote{Add a few examples of transformed pairs?}

\textit{Building a training set.} 1 million of crowdsourced annotations (see Section \ref{methods}) can be used to generate positive pairs $(x=(s_1, s_2), y=0)$ for all
$s_1, s_2 \in c_i^\prime$, for all $i$. Similarly, negative pairs $(x=(s_1,
s_2), y=1)$ can be extracted for all $s_1 \in c_i^\prime, s_2 \in c_j^\prime$, for
all $i \neq j$.

The most straightforward approach for building a training set on which to learn
a linkage function is to sample an equal number of positive and negative pairs,
as suggested above.
By observing that the linkage function $d$ will eventually
be used only on pairs of signatures from the same block $S_b$, a further
refinement for building a training set is to restrict positive and negative
pairs $(s_1, s_2)$ to only those for which $s_1$ and $s_2$ belong to the same
block. In doing so, the trained classifier is forced to learn intra-block
discriminative patterns rather than inter-block differences.
Furthermore, as noted in \cite{lange2011frequency}, most signature pairs are non-ambiguous:
if both signatures share the same author names, then
they correspond to the same individual, otherwise they do not.
Rather than sampling pairs uniformly at random, we propose to oversample
difficult cases when building the training set (\ie, pairs of signatures with
different author names corresponding to same individual, and pairs of
signatures with identical author names but corresponding to distinct
individuals) in order to improve the overall accuracy of the linkage function.

\subsection{Semi-supervised clustering}
\label{methods:clustering}

\longpage

The last component of our author disambiguation pipeline is clustering - 
the process of grouping together, within a block, all signatures from the same
individual (and only those).
As for many other works on author disambiguation, we make use of hierarchical clustering \cite{ward1963hierarchical} for
building clusters of signatures in a bottom-up fashion.
The method involves iteratively merging together the two most similar clusters until all clusters
are merged together at the top of the hierarchy.
Similarity between clusters is evaluated using either complete, single or average linkage, using
as a pseudo-distance metric the probability that $s_1$ and $s_2$ correspond to
distinct authors, as calculated from the custom linkage function $d$ from Section \ref{methods:linkage}.

To form flat clusters from the hierarchy, one must decide on a maximum
distance threshold above which clusters are considered to correspond to
distinct authors.
Let us denote by ${\cal S}^\prime = \{ s | s \in c^\prime, c^\prime
\in {\cal C}^\prime \}$ the set of all signatures for which partial clusters are
known.
Let us also denote by $\smash{\widehat{\cal C}}$ the predicted clusters for all signatures in ${\cal S}$, and by
$\smash{\widehat{\cal C}^\prime} = \{ \widehat{c} \cap {\cal S}^\prime | \widehat{c} \in \widehat{\cal C} \}$
the predicted clusters restricted to signatures for which partial clusters are known.
From these, we evaluate the following semi-supervised cut-off strategies, as illustrated in Figure~\ref{fig:cuts}:

\begin{itemize}

\item \textit{No cut:} all signatures from the same block are assumed to be from the same author.

\item \textit{Global cut:} the threshold is chosen globally over all blocks,
    as the one maximizing some score $f({\cal C}^\prime, \widehat{\cal C}^\prime)$.

\item \textit{Block cut:} the threshold is chosen locally at each block $b$,
    as the one maximizing some score $f({\cal C}_b^\prime, \widehat{\cal C}_b^\prime)$.
    In case ${\cal C}_b^\prime$ is empty, then all signatures from $b$ are clustered together.
\end{itemize}

\begin{figure*}
\centering
\includegraphics[width=\textwidth]{fig-cuts.pdf}
\caption{Semi-supervised cut-off strategies to form flat clusters of signatures.}
\label{fig:cuts}
\end{figure*}


% hierarchical clustering
% semi-supervised clustering
% cold start problem (when no labels in a block)

% \subsection{Matching}
% general description in a real system
% GL: out of scope, let us mention that in Conclusions instead
% Implementation ======================================================

\section{Implementation}
\label{implementation}

\longpage

As part of this work, we developed a stand-alone application for author
disambiguation, publicly available online\footnote{\url{https://github.com/glouppe/beard}} for free reuse
or study.
Our implementation builds upon the Python scientific stack, making
use of the Scikit-Learn library \cite{scikitlearn} for the supervised learning
of a linkage function and of SciPy for clustering.
All components of the disambiguation pipeline have been designed to follow the
Scikit-Learn API \cite{scikitlearnAPI}, making them easy to maintain,
understand and reuse.
Our implementation is made to be efficient, exploiting parallelization when available, and ready for production environments.
It is also designed to be runnable in an incremental fashion in which our approach is considered to be scalable. We adopt the blocking phase in order to reduce the computational complexity from $O(N^2)$ to $O(\sum N_i^2)$, which in practice tends to ${O(N)}$ when $N_i << N$. This also means that instead of having to run the disambiguation process on the whole signature set, the process could be run only on specified blocks if desired.

% Results and discussion ======================================================

\section{Experiments}
\label{experiments}

All the solutions proposed in this work are evaluated on data extracted from
the \emph{INSPIRE} portal \cite{gentil2009information}, a digital library for scientific literature in
high-energy physics.
Overall, the portal holds more than 1 million publications ${\cal P}$,
forming in total a set ${\cal S}$ of more than 10 million signatures.
Out of these, around 13\% have been \textit{claimed} by their
original authors, marked as such by professional curators or automatically assigned to their true authors thanks
to persistent identifiers provided by publishers or other sources.
Together, they constitute a trusted set $({\cal S}^\prime, {\cal C}^\prime)$ of 15388 distinct individuals sharing
36340 unique author names spread within 1201763 signatures on 360066
publications. This data covers several decades in time and dozens of author
nationalities worldwide.

Following the \emph{INSPIRE} terms of use, the signatures ${\cal S}^\prime$ and their
corresponding clusters ${\cal C}^\prime$ are released
online\footnote{\url{https://github.com/glouppe/paper-author-disambiguation}}
under the CC0 license.
To the best of our knowledge, data of this size and coverage is the first to be publicly
released in the scope of author disambiguation research.

\subsection{Evaluation protocol}

Experiments carried out to study the impact of the proposed algorithmic
components and refinements, follow a
standard 3-fold cross-validation protocol, using $({\cal S}^\prime, {\cal
C}^\prime)$ as ground-truth dataset. To replicate the $|{\cal S}^\prime| /
|{\cal S}| \approx 13\%$ ratio of claimed signatures with respect to the total
set of signatures, as on the INSPIRE platform, cross-validation folds are
constructed by sampling 13\% of claimed signatures to form a training set ${\cal
S}_\text{train}^\prime \subseteq {\cal S}^\prime$.
The remaining signatures ${\cal S}_\text{test}^\prime = {\cal S}^\prime \setminus {\cal
S}_\text{train}^\prime$ are used for testing.
Therefore, ${\cal C}_\text{train}^\prime = \{ c^\prime \cap {\cal S}_\text{train}^\prime | c^\prime \in {\cal C}^\prime
\}$ represents the partial known clusters on the training fold, while ${\cal
C}_\text{test}^\prime$ are those used for testing.

As commonly performed in author disambiguation research,
we evaluate the predicted clusters over testing data  ${\cal C}_\text{test}^\prime$,
using both B3 and pairwise precision, recall
and F-measure, as defined below:
\begin{small}
\begin{align}
P_\text{B3}({\cal C}, \widehat{\cal C}, {\cal S}) &= \frac{1}{|{\cal S}|} \sum_{s \in {\cal S}} \frac{|c(s) \cap \widehat{c}(s)|}{|\widehat{c}(s)|} \qquad
R_\text{B3}({\cal C}, \widehat{\cal C}, {\cal S}) = \frac{1}{|{\cal S}|} \sum_{s \in {\cal S}} \frac{|c(s) \cap \widehat{c}(s)|}{|c(s)|}\\
F_\text{B3}({\cal C}, \widehat{\cal C}, {\cal S}) &= \frac{2 P_\text{B3}({\cal C}, \widehat{\cal C}, {\cal S}) R_\text{B3}({\cal C}, \widehat{\cal C}, {\cal S})}{P_\text{B3}({\cal C}, \widehat{\cal C}, {\cal S}) + P_\text{B3}({\cal C}, \widehat{\cal C}, {\cal S})} \\
P_\text{pairwise}({\cal C}, \widehat{\cal C}) &= \frac{|p({\cal C}) \cap p(\widehat{\cal C})|}{|p(\widehat{\cal C})|} \qquad \qquad
R_\text{pairwise}({\cal C}, \widehat{\cal C}) = \frac{|p({\cal C}) \cap p(\widehat{\cal C})|}{|p({\cal C})|}\\
F_\text{pairwise}({\cal C}, \widehat{\cal C}) &= \frac{2 P_\text{pairwise}({\cal C}, \widehat{\cal C}) R_\text{pairwise}({\cal C}, \widehat{\cal C})}{P_\text{pairwise}({\cal C}, \widehat{\cal C}) + R_\text{pairwise}({\cal C}, \widehat{\cal C})}
\end{align}
\end{small}
and where $c(s)$ (resp. $\widehat{c}(s)$) is the cluster $c \in {\cal C}$ such that
$s \in c$ (resp. the cluster $\widehat{c} \in \widehat{\cal C}$ such that $s
\in \widehat{c}$), and where $p({\cal C}) = \cup_{c \in {\cal C}} \{ (s_1, s_2)
| s_1, s_2 \in c, s_1 \neq s_2 \}$ is the set of all pairs of signatures from
the same clusters in ${\cal C}$.
The F-measure is the harmonic mean between these two quantities.
In the analysis below, we rely primarily on the B3 F-measure for discussing results, as the pairwise variant
tends to favor large clusters (because the number of pairs is quadratic with the cluster size),
hence unfairly giving preference to authors with many publications.
By contrast, the B3 F-measure weights clusters linearly with respect to their size.
General conclusions drawn below remain however consistent for pairwise F.


\subsection{Results and discussion}

% Our approach could be considered as an experimental design solution. Our goal is
% to find the best combination of design parameters. Such a solution could be
% reached by performing many experiments where in each experiment we fix all
% design parameters except one. Of course, what we found as a best choice in each
% experiment will be used for the proceeding one and so on. To initialize the
% process, we will use what have been reported by some related works as a best
% method.

\longpage

\subsubsection{Baseline.} The simplest baseline against which we compare our results
consists in grouping all signatures sharing the same (normalized) surname(s) and
the same (normalized) first given name initial. It provides a simple and fast
solution yielding decent results, as reported at the top of
Table~\ref{table:results}.


\begin{small}
\begin{table*}
\caption{Average precision, recall and f-measure scores on test folds. - components correspond to the state-of-the-art choices.}
\label{table:results}
\centering
\begin{tabular}{|l|c c c | c c c|}
  \hline
                       & \multicolumn{3}{|c|}{\textbf{B3}} & \multicolumn{3}{|c|}{\textbf{Pairwise}}\\
  \textbf{Description} & $P$ & $R$ & $F$ & $P$ & $R$ & $F$ \\
  \hline
  \hline
Baseline & 0.9024 & 0.9828 & 0.9409  & 0.8298 & 0.9776 & 0.8977 \\
\hline
\underline{Blocking = SFI} & 0.9901 & 0.9760 & 0.9830  & 0.9948 & 0.9738 & 0.9842\\
Blocking = Double metaphone & 0.9856 & 0.9827 & 0.9841  & 0.9927 & 0.9817 & 0.9871 \\
Blocking = NYSIIS & 0.9875 & 0.9826 & \textbf{0.9850}  & 0.9936 & 0.9814 & \textbf{0.9875} \\
Blocking = Soundex & 0.9886 & 0.9745 & 0.9815 & 0.9935 & 0.9725 & 0.9828 \\
\hline
\underline{Classifier = GBRT} & 0.9901 & 0.9760 & 0.9830   & 0.9948 & 0.9738 & 0.9842 \\
Classifier = Random Forests & 0.9909 & 0.9783 & \textbf{0.9846}  & 0.9957 & 0.9752 & \textbf{0.9854} \\
Classifier = Linear Regression & 0.9749 & 0.9584 & 0.9666 & 0.9717 & 0.9569 & 0.9643 \\
\hline
Training pairs = Non-blocked, uniform & 0.9793 & 0.9630 & 0.9711  & 0.9756 & 0.9629 & 0.9692 \\
Training pairs = Blocked, uniform & 0.9854 & 0.9720 & 0.9786  & 0.9850 & 0.9707 & 0.9778 \\
\underline{Training pairs = Blocked, balanced} & 0.9901 & 0.9760 & \textbf{0.9830}  & 0.9948 & 0.9738 & \textbf{0.9842} \\
\hline
\underline{Clustering = Average linkage} & 0.9901 & 0.9760 & \textbf{0.9830}  & 0.9948 & 0.9738 & \textbf{0.9842} \\
Clustering = Single linkage & 0.9741 & 0.9603 & 0.9671  & 0.9543 & 0.9626 & 0.9584 \\
Clustering = Complete linkage & 0.9862 & 0.9709 & 0.9785  & 0.9920 & 0.9688 & 0.9803 \\
\hline
No cut (baseline) & 0.9024 & 0.9828 & 0.9409  & 0.8298 & 0.9776 & 0.8977 \\
Global cut & 0.9892 & 0.9737 & 0.9814  & 0.9940 & 0.9727 & 0.9832 \\
\underline{Block cut} & 0.9901 & 0.9760 & \textbf{0.9830}  & 0.9948 & 0.9738 & \textbf{0.9842} \\
\hline
Combined best settings & 0.9888 & 0.9848 & \textbf{0.9868}  & 0.9951 & 0.9831 & \textbf{0.9890} \\
Best settings without ethnicity features & 0.9862 & 0.9819 & 0.9841 & 0.9937 & 0.9815 & 0.9876 \\
  \hline
\end{tabular}
\end{table*}
\end{small}

\subsubsection{State-of-the-art.} Most methods proposed in related works have released
neither their software, nor their data, making a fair comparison very difficult.
Yet, we believe solutions reported in the literature can be closely
matched to our generic pipeline, provided the blocking strategy, the linkage
function and the clustering algorithm are properly aligned. In particular,
we consider hereon as the \textit{state-of-the-art} solution the following
combination of components:


\longpage

\begin{itemize}
	\item Blocking: same surname and the same first given name initial strategy (SFI);
	\item Linkage function: all 22 features defined in Table~\ref{table:features},
	gradient boosted regression trees as supervised learning algorithm
	and a training set of pairs built from $({\cal S}_\text{train}^\prime, {\cal C}_\text{train}^\prime)$, by balancing easy and difficult cases.
	\item Clustering: agglomerative clustering using average linkage and
	block cuts found to maximize $F_\text{B3}({\cal C}_\text{train}^\prime, \widehat{\cal C}_\text{train}^\prime, {\cal S}_\text{train}^\prime)$.
\end{itemize}

Below we study each component individually and discuss
results with respect to the underlined state-of-the-art solution.

\subsubsection{Blocking choices}
\label{choices:blocking}

\longpage

The good precision of the state-of-the-art ($0.9901$), but its
lower recall ($0.9760$) suggest that the blocking strategy might be the
limiting factor to further overall improvements.
Our experiments showed the maximum $B3$ recall (\ie, if within a block, all signatures were clustered optimally) for SFI is $0.9828$, which corroborates the
estimation of this technique on real data by \cite{torvik2009author}.
At the price of fewer and therefore slightly larger blocks, the
proposed phonetic-based blocking strategies show better maximum recall (all
around $0.9905$), thereby pushing further the upper bound on the maximum
performance of author disambiguation.
Let us remind however that the reported maximum recalls for the blocking strategies using phonetization are
also raised due to the better handling of multiple surnames, as described in Section \ref{methods:blocking}.

As Table~\ref{table:results} shows, switching to either Double metaphone or NYSIIS phonetic-based blocking allows
to improve the overall F-measure score.
In particular, the NYSIIS-based phonetic blocking shows to be the most effective when applied
to the state-of-the-art (with an F-measure of $0.9850$) while also being the most
efficient computationally (with 10857 blocks versus 12978 for the baseline).

%\begin{table*}
%\caption{Maximum recall $R_\text{B3}^*$ and $R_\text{pairwise}^*$ of blocking strategies, and %their number of blocks on ${\cal S}^\prime$.}
%\label{table:blocking}
%\centering
%\begin{tabular}{|l|cc|c|}
%  \hline
%  \textbf{Blocking} & $R_\text{B3}^*$ & $R_\text{pairwise}^*$ & \# blocks \\
%  \hline
%  \hline
%    SFI & 0.9828 & 0.9776 & 12978 \\
%    Double metaphone & 0.9907 & 0.9863 & 9753 \\
%    NYSIIS & 0.9902 & 0.9861 & 10857 \\
%    Soundex & 0.9906 & 0.9863 & 9403 \\
%  \hline
%\end{tabular}
%\end{table*}

%Finally, as discussed previously, the step of
%normalizing author names (stripping accents, removing affixes), as performed in the
%state-of-the-art, is shown to be important. Results from Table~\ref{table:results} clearly %suggest that not
%normalizing significantly reduces performance (yielding an F-measure of $0.9830$ when %normalizing,
%but decreasing to $0.9791$ when raw author name strings are used instead).

\subsubsection{Linkage function choices}
\label{choices:linkage}
Let us first comment on the results regarding the
supervised algorithm used to learn the linkage function.
As Table~\ref{table:results} indicates, both tree-based algorithms appear to be
significantly better fit than Linear Regression ($0.9830$ and $0.9846$ for GBRT
and Random Forests versus $0.9666$ for Linear Regression). This result is
consistent with \cite{treeratpituk2009disambiguating} which evaluated the use of
Random Forests for author disambiguation, but contradicts results of
\cite{levin2012citation} for which Logistic Regression appeared to be the best
classifier.
Provided hyper-parameters are properly tuned, the superiority of
tree-based methods is in our opinion not surprising.
Indeed, given the fact that the optimal linkage function is likely to be non-linear, non-parametric
methods are expected to yield better results, as the experiments here confirm.

Second, properly constructing a training set of positive and negative pairs of
signatures from which to learn a linkage function yields a significant
improvement.
A random sampling of positive and negative pairs, without taking
blocking into account, significantly impacts the overall performance
($0.9711$). When pairs are drawn only from blocks, performance increases
($0.9786$), which confirms our intuition that $d$ should be built only from
pairs it will be used to eventually cluster. Finally, making the classification
problem more difficult by oversampling complex cases proves to be relevant,
by further improving the disambiguation results ($0.9830$).

Moreover, we observed that the ethnicity features serve a purpose. When these features were not
included in the features set, using the best combined settings, the algorithm yields worse performance
($0.9841$).

Using Recursive Feature Elimination \cite{guyon2002gene}, we next evaluate the
usefulness of all fifteen standard and seven additional ethnicity features for learning
the linkage function. The analysis consists in using the state-of-the-art algorithm
first using all twenty two features, to determine the least discriminative from feature
importances \cite{louppe2013understanding}, and then re-learn the state-of-the-art algorithm
using all but that one feature. That process is repeated recursively until
eventually only one feature remains. Results are presented in
Figure~\ref{fig:rfe} for one of the three folds with
the state-of-the-art, starting from the far right, \textit{Second given name} being the least important feature,
and ending on the left with all features eliminated but \textit{Chinese}. As
the figure illustrates, the most important features are ethnic-based features
(\textit{Chinese}, \textit{Other Asian}, \textit{Black}) along with
\textit{Co-authors}, \textit{Affiliation} and \textit{Full name}. Adding the remaining
other features only brings marginal improvements, with \textit{Journal},
\textit{Abstract}, \textit{Collaborations}, \textit{References}, \textit{Given
name initial} and \textit{Second given name} being almost insignificant.
Overall, these results highlight the added value of the proposed ethnicity
features.
Their duality in modeling both the similarity between author names
and their origins make them very strong predictors for author disambiguation.
The results also corroborate those from \cite{kang2009co} or \cite{ferreira2010effective}, who
found that the similarity between co-authors was a highly discriminative
feature.
If computational complexity is a concern, this analysis also
shows how decent performance can be achieved using only a very
small set of features, as also observed in
\cite{treeratpituk2009disambiguating} or \cite{levin2012citation}.


\begin{figure*}
	\centering
	\caption{Recursive Feature Elimination analysis. }
	\label{fig:rfe}
	\includegraphics[width=\linewidth]{fig-rfe.pdf}
\end{figure*}


\subsubsection{Semi-supervised clustering choices}
\label{choices:clustering}
The last part of our experiment concerns
the study of agglomerative clustering and the best way to find a cut-off
threshold to form clusters. Results from Table~\ref{table:results}
first clearly indicate that average linkage is significantly better than
both single and complete linkage.

\longpage

Clustering together all signatures from the same block (\ie, baseline) is the least effective
strategy ($0.9409$), but yields anyhow surprisingly decent accuracy, given the
fact it requires no linkage function and no agglomerative clustering -- only the blocking
function is needed to group signatures. In particular, this result reveals
that author names are not ambiguous in most cases\footnote{This holds for the
data we extracted, but may in the future, with the rise of non-Western
researchers, be an underestimate of the ambiguous cases.} and that only a small
fraction of them requires advanced disambiguation procedures. On the other
hand, both global and block cut thresholding strategies give better results,
with a slight advantage for the block cuts ($0.9814$ versus $0.9830$), as expected.
In case ${\cal S}^\prime_b$ is empty (\ie partial clusters are not known for any of the signatures from the
block), this therefore suggests that either using
a cut-off threshold learned globally from the known data would in general give results only
marginally worse than if the claimed signatures had been
known.

\subsubsection{Combined best settings} When all best settings are combined (\ie, Blocking = NYSIIS, Classifier = Random Forests, Training
pairs = blocked and balanced, Clustering = Average linkage, Block cuts),
performance reaches $0.9862$, \ie, the best of all reported results. In particular,
this combination exhibits both the high recall of phonetic blocking based on the NYSIIS algorithm and the high precision of Random Forests.

\subsubsection{Execution time} Our implementation takes around $20$ hours to process the complete set of the data (for 10M signatures, on a $16$ cores machine with $32$GB of RAM). Related work \cite{khabsa2014large} reports execution times around $24$ hours to cluster $4$M signatures. Note also that shorter execution times can be achieved, at the expense of worse results, by reducing the set of the features used.


% Conclusions =================================================================

\section{Conclusions}
\label{conclusions}

\longpage

In this work, we have revisited and validated the general author disambiguation
pipeline introduced in previous independent research work.
The generic approach is composed of three components, whose design and tuning are all critical
to good performance: (i) a blocking function for pre-clustering signatures
and reducing computational complexity, (ii) a linkage function for identifying
signatures with coreferring authors and (iii) the agglomerative clustering of
signatures. Making use of a distinctively large dataset of more than 1 million
crowdsourced annotations, we experimentally study all three components and
propose further improvements. With regards to blocking, we suggest to use
phonetization of author names to increase recall while maintaining low
computational complexity. For the linkage function, we introduce
ethnicity-sensitive features for the automatic tailoring of disambiguation to non-Western
author names whenever necessary. Finally, we explore semi-supervised cut-off
threshold strategies for agglomerative clustering. For all three components,
experiments show that our refinements all yield significantly better author
disambiguation accuracy.

Overall, these results all encourage further improvements and research. For
blocking, one of the challenges is to manage signatures with inconsistent
surnames or first given names (cases 4 and 5, as described in
Section~\ref{methods:blocking}) while maintaining blocks to a tractable size.
As phonetic algorithms are not yet perfect, another direction  for further work is the design of better
phonetization functions, tailored for author disambiguation. For the linkage function,
the good results of the proposed features pave the way for further research  in
ethnicity-sensitive author disambiguation. The automatic fitting of the
pipeline to cultures and ethnic groups for which standard author disambiguation
is known to be less efficient (\eg, Chinese authors with many homonyms)  indeed
constitutes a direction of research with great potential benefits for the
concerned scientific communities.

Moreover, the techniques presented in this work can be easily adapted to the broader
problem of named entity disambiguation and might improve accuracy of semantic search
algorithms.

As part of this study, we also publicly release the annotated data extracted
from the \emph{INSPIRE} platform, on which our experiments are based.
To the best of our knowledge, data of this size and coverage is the first to be
available in author disambiguation research. By releasing the data publicly,
we hope to provide the basis for further research on author disambiguation
and related topics.

\verylongpage

\bibliographystyle{abbrv}
\bibliography{bib}

\end{document}
